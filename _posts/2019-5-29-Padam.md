---
layout: post
title: Adaptive exponent for adam (padam)
---

One long-standing issue preventing the complete adoption of adaptive optimization methods for training neural networks is the generalization gap between adaptive optimization methods and stochastic gradient descent with momentum (sgd+mom). Using adaptive methods, such as Adaptive Moment Estimation (Adam) which I will introduce later, with default values or with slight tuning usually yields good performance. This may sometimes be good enough, and at the very least serves as a useful baseline. However, the performance of adaptive methods on validation or test sets especially with regards to image data is often somewhat worse than can be achieved with sgd+mom. This is clearly reflected in the top papers pushing the state-of-the-art image classification error for imagenet, where sgd+mom is almost universally used. An important objective is to close the generalization gap between adaptive methods and sgd+mom, thus reducing the amount of effort spent tuning and at the same time squeezing the most performance out of the architecture in question.

Adam is the most popular adaptive optimization method and is the latest extension of Adagrad, an adaptive optimization first introduced by Google in 2011. Interestingly enough, Adam was first published in 2014, and there has been no obvious successor since. In comparison, we've seen Resnet, Densenet, Mobilenet etc. series of novel neural network architectures in those last 5 years. 

To motivate Adam, I first introduce the update rule of Adagrad below, where g is the current gradient with respect to a particular weight.

<a href="https://www.codecogs.com/eqnedit.php?latex=\theta_{t&space;&plus;&space;1}&space;=&space;\theta_{t}&space;-&space;\frac{\eta}{\sqrt{v_t}&space;&plus;&space;\epsilon}g_t&space;\:&space;." target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_{t&space;&plus;&space;1}&space;=&space;\theta_{t}&space;-&space;\frac{\eta}{\sqrt{v_t}&space;&plus;&space;\epsilon}g_t&space;\:&space;," title="\theta_{t + 1} = \theta_{t} - \frac{\eta}{\sqrt{v_t} + \epsilon}g_t \: ," /></a>

where

<a href="https://www.codecogs.com/eqnedit.php?latex=v_{t&space;&plus;&space;1}=v_t&space;&plus;&space;g_t^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_{t&space;&plus;&space;1}=v_t&space;&plus;&space;g_t^2&space;\:&space;." title="v_{t + 1}=v_t + g_t^2 \: ." /></a>

The intuition behind the v term is to accumulate values such that weights which are rarely updated are updated with a larger value when they do receive an update, and weights which are frequently updated are updated with a smaller value. A natural question may be why take the square root of v in the update? As it turns out, taking the square root of v works better empirically. Another natural question may be why accumulate squared gradients? I've empirically tested accumulating values using abs(g) or counters with vgg-16 and resnet-20 on the cifar datasets and they don't work nearly as well. This is due to a theoretical motivation for accumulating squared gradients. The Fisher information matrix .... Accumulating squared gradients is the empirical estimation for the variance

Are we done? Well, v appears to grow monotonically and what happens if we add momentum? We arrive at Adam. Adam maintains a running exponential average for v to prevent v from growing indefinitely also incorporates a momentum term. The formula is given below, omitting a correction term for simplicity.

<a href="https://www.codecogs.com/eqnedit.php?latex=\theta_{t&space;&plus;&space;1}&space;=&space;\theta_{t}&space;-&space;\frac{\eta}{\sqrt{v_t}&space;&plus;&space;\epsilon}m_t&space;\:&space;." target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_{t&space;&plus;&space;1}&space;=&space;\theta_{t}&space;-&space;\frac{\eta}{\sqrt{v_t}&space;&plus;&space;\epsilon}m_t&space;\:&space;" title="\theta_{t + 1} = \theta_{t} - \frac{\eta}{\sqrt{v_t} + \epsilon}m_t \: " /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=v_{t&space;}&space;=&space;\beta_2v_{t&space;-&space;1}&space;&plus;&space;(1&space;-&space;\beta_2)g_t^2&space;," target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_{t&space;}&space;=&space;\beta_2v_{t&space;-&space;1}&space;&plus;&space;(1&space;-&space;\beta_2)g_t^2&space;" title="v_{t } = \beta_2v_{t - 1} + (1 - \beta_2)g_t^2 " /></a>

Finally, 

This is precisely what is proposed in Closing the generalization gap by Quanquan Gu, a professor at UCLA.

![_config.yml]({{ site.baseurl }}/images/config.png)

The easiest way to make your first post is to edit this one. Go into /_posts/ and update the Hello World markdown file. For more instructions head over to the [Jekyll Now repository](https://github.com/barryclark/jekyll-now) on GitHub.
