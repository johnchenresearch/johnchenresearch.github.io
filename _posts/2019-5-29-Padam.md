---
layout: post
title: Adaptive exponent for adam (padam)
---

One long-standing issue preventing the complete adoption of adaptive optimization methods for training neural networks is the generalization gap between adaptive optimization methods and stochastic gradient descent with momentum (sgd+mom). Using adaptive methods, such as Adaptive Moment Estimation (Adam) which I will introduce later, with default values or with slight tuning usually yields good performance. This may sometimes be good enough, and at the very least serves as a useful baseline. However, the performance of adaptive methods on validation or test sets especially with regards to image data is often somewhat worse than can be achieved with sgd+mom. This is clearly reflected in the top papers pushing the state-of-the-art image classification error for imagenet, where sgd+mom is almost universally used. An important objective is to close the generalization gap between adaptive methods and sgd+mom, thus reducing the amount of effort spent tuning and squeezing the most performance out of the architecture in question.

Adam is the most popular adaptive optimization method and is the latest extension of Adagrad, an adaptive optimization first introduced by Google in 2011. Interestingly enough, Adam was first published in 2014, and there has been no obvious successor since. In comparison, we've seen Resnet, Densenet, Mobilenet etc. series of novel neural network architectures in those last 5 years. To motivate Adam, I first introduce Adagrad below:



empirical estimate of the variance of the fisher information matrix

![_config.yml]({{ site.baseurl }}/images/config.png)

The easiest way to make your first post is to edit this one. Go into /_posts/ and update the Hello World markdown file. For more instructions head over to the [Jekyll Now repository](https://github.com/barryclark/jekyll-now) on GitHub.
